{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b5880c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense,LSTM\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from textblob import TextBlob\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "677d4336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Frequent underground bombings in my city. I wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@kharge Respected leader of opposition the ele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mr. @mkstalin Today SC slapped you hard on you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The meeting was attended by ACS, MAWS; Pr. Sec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@amarprasadreddy @mkstalin Time Has Come Once ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet\n",
       "0  Frequent underground bombings in my city. I wa...\n",
       "1  @kharge Respected leader of opposition the ele...\n",
       "2  Mr. @mkstalin Today SC slapped you hard on you...\n",
       "3  The meeting was attended by ACS, MAWS; Pr. Sec...\n",
       "4  @amarprasadreddy @mkstalin Time Has Come Once ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3df993fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweet=df['Tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74c839a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_tweets=[]\n",
    "labels=[]\n",
    "for tweet in Tweet:\n",
    "    blob = TextBlob(tweet)\n",
    "    sentiment = blob.sentiment.polarity  # Sentiment polarity ranges from -1 to 1\n",
    "    labeled_tweets.append(tweet)\n",
    "    labels.append(1 if sentiment >= 0 else 0)  # Assign label 1 for positive sentiment, 0 for negative sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7a33e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Feature Extraction\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(labeled_tweets)\n",
    "sequences = tokenizer.texts_to_sequences(labeled_tweets)\n",
    "word_index = tokenizer.word_index\n",
    "max_length = max(len(seq) for seq in sequences)\n",
    "data = pad_sequences(sequences, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a0c3f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Split the Data\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60abd529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "63/63 [==============================] - 12s 149ms/step - loss: 0.4515 - accuracy: 0.8313 - val_loss: 0.4100 - val_accuracy: 0.8415\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 9s 144ms/step - loss: 0.3667 - accuracy: 0.8474 - val_loss: 0.3445 - val_accuracy: 0.8535\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 10s 157ms/step - loss: 0.1694 - accuracy: 0.9344 - val_loss: 0.3166 - val_accuracy: 0.8785\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 10s 154ms/step - loss: 0.0507 - accuracy: 0.9847 - val_loss: 0.3471 - val_accuracy: 0.8790\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 9s 136ms/step - loss: 0.0175 - accuracy: 0.9960 - val_loss: 0.3959 - val_accuracy: 0.8855\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 9s 141ms/step - loss: 0.0077 - accuracy: 0.9989 - val_loss: 0.4176 - val_accuracy: 0.8885\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 10s 151ms/step - loss: 0.0040 - accuracy: 0.9995 - val_loss: 0.4549 - val_accuracy: 0.8880\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 10s 164ms/step - loss: 0.0025 - accuracy: 0.9998 - val_loss: 0.4796 - val_accuracy: 0.8845\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 10s 163ms/step - loss: 0.0017 - accuracy: 0.9999 - val_loss: 0.5022 - val_accuracy: 0.8840\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 10s 152ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.5112 - val_accuracy: 0.8870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2da3b700e80>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to NumPy arrays\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Step 5: Model Training\n",
    "embedding_dim = 100  # Define the dimensionality of word embeddings\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, embedding_dim, input_length=max_length))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a419826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 1s 14ms/step - loss: 0.5112 - accuracy: 0.8870\n",
      "Test Loss: 0.5111795663833618, Test Accuracy: 0.8870000243186951\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Model Evaluation\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4574bfa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 1s 13ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.51      0.59       317\n",
      "           1       0.91      0.96      0.93      1683\n",
      "\n",
      "    accuracy                           0.89      2000\n",
      "   macro avg       0.80      0.73      0.76      2000\n",
      "weighted avg       0.88      0.89      0.88      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_prob = model.predict(x_test)  # Predicted probabilities\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)  # Convert probabilities to class labels (0 or 1)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print the classification report\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2b51b76",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_tweets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12376\\1384591214.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Step 7: Sentiment Prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Assume you have new, unlabeled tweets stored in `new_tweets`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mnew_sequences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_tweets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mnew_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_sequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnew_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'new_tweets' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 7: Sentiment Prediction\n",
    "# Assume you have new, unlabeled tweets stored in `new_tweets`.\n",
    "new_sequences = tokenizer.texts_to_sequences(new_tweets)\n",
    "new_data = pad_sequences(new_sequences, maxlen=max_length)\n",
    "new_data = np.array(new_data)\n",
    "sentiment_predictions = model.predict(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4212b210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Aggregating Predictions\n",
    "positive_count = np.sum(sentiment_predictions > 0.5)\n",
    "negative_count = len(sentiment_predictions) - positive_count\n",
    "print(f\"Positive Count: {positive_count}, Negative Count: {negative_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c41229d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Split the Data\n",
    "x_train1, x_test1, y_train1, y_test1 = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "# Convert to NumPy arrays\n",
    "x_train1 = np.array(x_train1)\n",
    "x_test1 = np.array(x_test1)\n",
    "y_train1 = np.array(y_train1)\n",
    "y_test1 = np.array(y_test1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "529c8887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "63/63 [==============================] - 42s 575ms/step - loss: 0.4475 - accuracy: 0.8288 - val_loss: 0.3835 - val_accuracy: 0.8465\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 36s 572ms/step - loss: 0.2687 - accuracy: 0.8939 - val_loss: 0.3271 - val_accuracy: 0.8605\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 34s 545ms/step - loss: 0.1018 - accuracy: 0.9654 - val_loss: 0.3953 - val_accuracy: 0.8765\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 33s 530ms/step - loss: 0.0403 - accuracy: 0.9871 - val_loss: 0.4230 - val_accuracy: 0.8745\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 32s 513ms/step - loss: 0.0180 - accuracy: 0.9950 - val_loss: 0.4611 - val_accuracy: 0.8715\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 33s 524ms/step - loss: 0.0150 - accuracy: 0.9959 - val_loss: 0.5165 - val_accuracy: 0.8760\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 33s 522ms/step - loss: 0.0068 - accuracy: 0.9984 - val_loss: 0.6166 - val_accuracy: 0.8700\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 34s 539ms/step - loss: 0.0044 - accuracy: 0.9995 - val_loss: 0.6547 - val_accuracy: 0.8760\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 35s 558ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.7019 - val_accuracy: 0.8615\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 32s 505ms/step - loss: 7.1607e-04 - accuracy: 1.0000 - val_loss: 0.7700 - val_accuracy: 0.8650\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2da3c041970>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5: Model Training\n",
    "embedding_dim = 100  # Define the dimensionality of word embeddings\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(len(word_index) + 1, embedding_dim, input_length=max_length))\n",
    "model2.add(LSTM(128))\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model2.fit(x_train1, y_train1, validation_data=(x_test1, y_test1), epochs=10, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74d7c1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 5s 76ms/step - loss: 0.7700 - accuracy: 0.8650\n",
      "Test Loss: 0.7699757218360901, Test Accuracy: 0.8650000095367432\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Model Evaluation\n",
    "loss2, accuracy2 = model2.evaluate(x_test1, y_test1)\n",
    "print(f\"Test Loss: {loss2}, Test Accuracy: {accuracy2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab50c581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 7s 78ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.55      0.56       317\n",
      "           1       0.92      0.93      0.92      1683\n",
      "\n",
      "    accuracy                           0.86      2000\n",
      "   macro avg       0.75      0.74      0.74      2000\n",
      "weighted avg       0.86      0.86      0.86      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_prob1 = model2.predict(x_test1)  # Predicted probabilities\n",
    "y_pred1 = (y_pred_prob1 > 0.5).astype(int)  # Convert probabilities to class labels (0 or 1)\n",
    "report1 = classification_report(y_test1, y_pred1)\n",
    "\n",
    "# Print the classification report\n",
    "print(report1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9156707f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Sentiment Prediction\n",
    "# Assume you have new, unlabeled tweets stored in `new_tweets`.\n",
    "new_sequences = tokenizer.texts_to_sequences(new_tweets)\n",
    "new_data = pad_sequences(new_sequences, maxlen=max_length)\n",
    "sentiment_predictions = model.predict(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f709f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Aggregating Predictions\n",
    "positive_count = np.sum(sentiment_predictions > 0.5)\n",
    "negative_count = len(sentiment_predictions) - positive_count\n",
    "print(f\"Positive Count: {positive_count}, Negative Count: {negative_count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
